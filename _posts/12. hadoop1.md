# hadoop

\* hadoop 2.6.0 버전을 기준으로 작성하였습니다.

## hadoop 이란?

* 빅데이터 분산처리를 위한 오픈소스 프레임워크.
* 하나의 성능 좋은 컴퓨터를 이용하여 데이터를 처리하는 대신, 적당한 성능의 범용 컴퓨터 여러 대를 클러스터화하고, 빅데이터를 클러스터에서 분산 처리하여 처리 속도를 높이는 것을 목적으로 함
* 하둡은 기존의 RDBMS을 대치하는 것이 아님
* 트랜잭션이나 무결성을 보장해야하는 데이터처리에는 적합하지 않음
* 배치성으로 데이터를 저장하고 처리하는데에 적합한 시스템
* 장점
    * scale out이 용이하다
    * Fault tolerance가 높다.
        * Block Replication 기능
            * 복제본들을 복수의 rack에 저장
            * data read operation 시 checksum을 통한 검증
        * standby namenode를 통한 namenode metadata 유실 방지
* 단점
    * HDFS에 저장된 데이터를 변경하는 것이 불가.
    * 신속한 데이터 처리가 어려움(HDFS는 높은 데이터 처리량을 제공하기 위해 최적화 되어있고 이를 위해 응답시간을 희생했다.)
    * 다수의 작은 파일로 처리되는 작업
* hadoop의 4가지 핵심 구성 요소는 HDFS, map-reduce, yarn, hadoop common
    * HDFS(Hadoop Distributed File System)
        * master slave 아키텍처로 되어 있는 hadoop storage system.
        * master node는 NameNode 이며, 파일 시스템의 메타데이터를 관리한다. master node에는 job tracker가 있어서 slave의 map reduce job을 관리한다.
        * slave node들은 DataNode로서 실제 데이터를 저장한다. 각각의 slave node에는 task tracker 가 있고 데이터 처리를 수행하며 그 결과를 master node에 전달한다.
    * map-reduce
        * 분산 처리(연산)를 수행한다.
    * YARN(<span style="color:  #4d5156;;">Yet Another Resource Negotiator</span>)
        * job 스케줄링, 클러스터 리소스 관리 수행
    * hadoop common
        * 자바 라이브러리와 유틸리티들을 및 hadoop을 실행시키기 위한 파일들을 포함

## HDFS(Hadoop Distributed File System)

### Architecture

hdfs는 master/slave 구조로써, 하나의 namenode와 다수의 datanode로 구성된다.

### namenode

namenode는 파일시스템 네임스페이스를 관리하고 클라이언트로부터 파일요청을 중재하는 역할을 하는 마스터 서버이다.
네임스페이스는 파일시스템에 대한 메타데이터이다.
namenode가 구동 될 때 meta data는 memory에 적재 되며 영구적인 저장을 위해 디스크에도 저장되어있다.
이 메타데이터와 관련된 파일은 아래 두 가지이다.
<br>
* fsimage - hdfs filesystem에 대한 snapshot이며 namenode가 start up 할 때 읽어들인다.
* edit logs - namenode start up 이후 filesystem에 가해지는 modification들을 기록한 파일

filesystem에 modification이 생길 경우 이는 edit log 파일에 기록이 되고
namenode가 re-startup 할 때, fsimage 파일을 읽은 후 여기에 edit logs 파일의 내용을 반영하여
새로운 fsimage로 갱신하는 과정을 거친다.(edit logs 파일은 빈파일이 됨)

namenode는 파일과 디렉토리에 대한 open, close, rename 등의 '파일시스템 네임스페이스' 명령들을 수행한다.
또한 datanode에 블록을 매핑한다.

### Secondary namenode

namenode가 start up 할 때, FsImage파일에서 hdfs의 상태를 읽고 Edits Log파일로 부터 edit내역을 반영한다.
그리고 이렇게 만들어진 새로운 hdfs 상태를 FsImage파일에 쓰고 새로운 빈 edit 파일을 가지고 start up을 진행한다.
즉, namenode는 start up 시에만 FsImage와 Edit Logs를 병합하기에, 클러스터 장기 구동 시 Edit Logs 파일은 매우 커질 수 있다.
이로 인한 문제는, namenode가 restart하게 될 경우 시간이 오래 걸릴 수 있다는 점.

이를 해결하기 위해 Secondary namenode는 edit log data를 fsimage파일에 추가하며 edit log file의 크기를 일정 수준 이하로 제한해 주는 역할을 수행한다.
또한 namenode 파일시스템 metadata에 문제가 생겼을 때 metadata를 백업하는 기능도 수행할 수 있다.

Secondary namenode는 메타데이터를 지속적으로 병합하고 갱신한다.
이를 체크포인트 과정이라고 하며, fsImage를 메모리로 반영하기 때문에 Secondary namenode도 main namenode와 비슷한 메모리 spec으로 구성하는 것이 권장된다.

Secondary namenode의 동작과정은 다음과 같다.
main namenode에 있는 editLog와 fsImage를 가져온 뒤,  fsImage를 메모리에 올리고 그 후 editLog의 내용을 반영한다.
그리고 나서 새롭게 통합된 FsImage파일을 생성하고 이를 main namenode에 전송한다.
main namenode는 이전 fsImage를 새롭게 전달받은 fsImage로 교체하고, edit Log 파일을 새로운 빈 edit Log파일로 교체한다.
(체크포인트가 진행되는 상황에서도 edit이 발생할 수 있기 때문에 정확히 빈 Edit Log 파일은 아닐 수 있다.)

\* hadoop HA를 구성하게 되면 standby namenode가 있어서 secondary namenode는 필요 없다.

### datanode

hdfs 내부적으로 파일은 하나 이상의 블록으로 나뉘어지고 이 블록들은 datanode들에 저장된다.
datanode는 파일시스템의 클라이언트로부터의 read/write 요청을 수행하며 또한 namenode의 명령하에 블록의 생성, 삭제, 복제도 담당한다.
하나의 머신에 다수의 datanode를 기동 시킬 수 있으나, 통상적으로는 하나만 기동시킨다.
datanode는 해당 datanode가 있는 머신에 할당된 저장소를 관리한다.
datanode는 주기적으로 저장하고 있는 블록의 리스트를 namenode로 보고한다.
![image](https://user-images.githubusercontent.com/33619494/188454986-83736d39-9ec1-4de6-83a2-ea97dfd45233.png)
<span style="color:  #999999;;">그림 1 HDFS Architecture [출처](https://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html)</span>

<br>
namenode에 의해 관리되며 HDFS의 메타데이터를 담고 있는 저장소이다.
namespace 혹은 그에 대한 속성의 변화는 전부 namenode에 의해 기록된다.
파일시스템 트리와 그 트리 안에 있는 모든 파일과 디렉터리에 대한 메타데이터를 유지한다.
hdfs에 파일이 추가 될 때 namenode는 파일의 이름을 파일 시스템 트리에 추가하고 이를 위한 데이터 블록을 매핑한다.
fsImage와 editlogs라는 파일로 관리된다.

### Data Replication

hdfs는 클러스터를 구성하는 여러 머신들에 걸쳐 굉장히 큰 용량의 파일을 저장할 수 있다.
hdfs는 각 파일을 여러 블록들에 저장하는데, 마지막 블록을 제외한 모든 블록들은 같은 크기를 갖는다.
이때, 블록들에 결함이 생기는 경우를 대비해 블록들은 복제되어 저장된다.
파일이 갖고 있는 복제본의 수를 그 파일의 replication factor라고 하며 이 정보는 namenode에 의해 관리된다.
블록 크기나 replication factor는 파일마다 다르게 구성할 수 있다.
replication factor는 파일 생성 시에 명시되며 추후 변경 가능하다.

datanode들은 namenode에게 주기적으로 heartbeat, block에 대한 report를 전송한다.
<br>
* heartbeat

namenode가 heartbeat을 수신했다고 함은 datanode가 정상적으로 동작하는 것을 암시하는 것이다.
<br>
* block report

block report 를 통해 해당 datanode에 있는 모든 블록에 대한 리스트를 namenode에 보여주게 된다.
namenode는 블록들의 실제 위치를 저장하지 않으며 이 block report를 통해 파악한다.

![image](https://user-images.githubusercontent.com/33619494/188455507-5da8e5f9-16b0-48dd-b708-9d7289beb72f.png)
<span style="color:  #999999;;">그림 2 datanode block replication [출처](https://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html)</span>

### Replica Placement

블록 복제본의 배치는 데이터의 신뢰성, 유효성 그리고 네트워크 대역폭 활용성을 향상시키는데에 있어 중요한 문제이다.
 hdfs 클러스터는 규모에 따라 다르지만 수많은 rack 들로 구성 될 수 있다.
rack이라 함은 네트워크 스위치를 공유하는 하나의 통신망의 의미이다.
다른 rack에 속한 두 데이터 노드간 통신을 위해서는 스위치를 경유하여야 하는데
같은 rack에 속한 두 머신간의 통신에 비해 다른 rack에 속한 두 머신간의 통신에서 더 많은 네트워크 리소스를 사용하게 된다.
이때 hdfs는 복제본을 최대한 unique 한 rack에 배치시킨다.

unique한 rack이라는 것은 아래와 같은 조건을 만족하는 rack을 의미한다.
<br>
* A 라는 데이터 복제본을 배치하려 하는 상황
* 이때 rack B에 속한 datanode들은 A라는 복제본을 가지고 있지 않은 상태

<br>
이렇게 함으로써 특정 rack 전체에 fail상태가 발생했을 경우에도 data 손실을 방지할 수 있으며
네트워크 대역폭 관점에서도 여러 rack에서 data read 동작이 가능해진다.
즉, datanode들에 data를 최대한 분산 저장함으로서 load balancing의 효과도 얻을 수 있다.

### namenode의 safe mode

namenode는 start up 하면서 fsImage로 부터 파일 시스템 상태를 로드하고 Edit Log 파일의 내용을 반영한다.
그 후 datanode들이 블록정보를 보내는 것을 기다린다.
이는 충분한 복제 블록들이 존재 함에도 불필요한 복제 작업을 진행하지 않도록 하기 위함이다.
이 기다리는 상태를 safe mode라고 하며 이 동안 namenode는 hdfs 에 대해 read-only 상태가 되고 file system, block에 수정 작업 불가하다.
datanode들이 정보 보내는 것을 마치면 safe mode는 종료된다.

### File System Metadata의 영속적인 관리(fsImage, editLog)
<br>
* fsImage

fsImage는 hdfs 메타데이터의 완전하고 영속적인 체크포인트이자 스냅샷이다.
fsImage는 파일시스템에 존재하는 모든 디렉터리와 파일 아이노드 정보를 바이트로 직렬화(디렉터리, 파일명, 상태정보 등을 바이트 배열로 표현)한 파일이다.
각 아이노드는 파일이나 디렉터리의 내부적인 표현이며 파일의 복제 단위, 변경 및 접근 시간, 접근 권한, 블록 크기와 파일을 구성하는 블록 조합들 같은 정보를 가진다.
파일과 블록의 매핑, 파일시스템 속성을 포함한 전체 파일시스템 네임스페이스는 fsImage라는 파일에 저장된다.
fsImage는 매 write opreation 마다 갱신 되지는 않는다.
Edit log에 어느정도 데이터가 쌓이거나 일정 주기가 되면 Edit log에 있는 내용을 FsImage로 적재한다.

fsImage파일은 블록이 저장되는 datanode 정보를 기록하지는 않는다.
대신 namenode는 메모리상에 이 블록과 datanode간의 매핑 정보를 유지한다.
namenode는 datanode가 클러스터에 참여할 때, 블록 리스트를 요청하여 매핑 정보를 구성하며,
이후에도 주기적으로 블록 정보를 받아 매핑 정보를 최신 상태로 유지한다.

<br>
* editLog

namenode는 지속적으로 파일시스템 메타데이터에 가해지는 변경을 기록하기 위해 editLog라고 불리는 transaction log를 활용한다.
write operation을 수행할 때마다, 제일 먼저 이를 editLog에 기록한다.
namenode는 파일시스템 메타데이터를 메모리로 올려서 인-메모리 자료구조로 관리하며 editLog가 수정된 후에 업데이트를 한다.
인-메모리 메타데이터는 읽기 요청을 수행하는 데 사용된다.

예를 들어 hdfs에 새로운 파일을 생성하면, namenode는 이를 나타내는 레코드를 editLog에 삽입한다.
마찬가지로 replication factor의 수정시에도 EditLog에 새로운 레코드가 삽입된다.

namenode는 여러개의 디렉터리에 쓰기 동작을 마치고 클라이언트에 결과를 반환하기 전에 변경 값을 모든 editLog 복제본에 flush하고 동기화한다.

<br>


###
Datanode failure - heartbeat and re-replication
각 datanode는 heartbeat message를 namenode에게 주기적으로 전송한다.
namenode는 이 신호로 datanode의 결함 유무를 파악할 수 있다.
namenode는 최근 heartbeat message를 전송하지 않은 datanode를 dead상태로 보고 더 이상 그 노드에 새로운 IO request를 보내지 않는다.
dead상태인 datanode에 저장된 데이터는 더 이상 hdfs에서 사용 불가하다.

datanode의 death가 발생하면 몇몇 블록들의 replication factor가 감소할 수 있다.
namenode는 지속적으로 어떤 블록들이 복제되어야 하는지 추적하고 필요에 따라 복제를 다시 진행한다.


* 재 복제가 일어날 수 있는 요인
    * datanode unavailable 상태
    * 복제본의 수정
    * datanode의 disk의 장애
    * replication factor의 변경
    * 기타 등등

### data integrity

저장소의 결함, 네트워크 문제, software상의 버그 등으로 datanode로 부터 받은 데이터가 정확하지 않을 수 있다.
이러한 점을 보완하기 위해 hdfs 클라이언트에서 파일을 생성할 때, 이 파일의 블록들에 대한 checksum값을 같이 계산 하여 별도의 hdfs namespace의 hidden file에 저장한다.
그리고 다시 클라이언트에서 이 파일 데이터를 요청할 시 각 datanode에서 얻은 checksum값을 checksum file에 저장된 값과 비교한다.
값이 다를 경우 클라이언트는 복제본을 가지고 있는 다른 datanode에 데이터를 요청한다.


### \- Metadata Disk Failure

fsImage와 editLog 는 hdfs의 핵심 데이터 구조다.
이 파일들에 결함이 발생하면 hdfs 시스템이 사용불가 상태가 될 수 있다.
따라서 namenode는 여러개의 fsImage, editLog 복사본을 관리하게 끔 구성 될 수 있다.
두 데이터는 하나가 갱신이 되면 다른 복제본들이 동기적으로 갱신된다.

### Data Oraganization

hdfs는 파일을 블록으로 쪼개서 저장한다.
일반적인 블록 사이즈는 64MB이기에 hdfs file은 64MB의 chunk로 분할되며 최대한 이 chunk들은 다른 datanode에 배치된다.

### staging

클라이언트의 파일 생성 요청은 바로 namenode를 거치는 것이 아니다.
hdfs 클라이언트는 처음에 file data를 임시 로컬 파일에 caching해 놓는다.

application의 쓰기 요청은 이 로컬 파일로 이루어진다.
로컬 파일이 담고 있는 데이터의 크기가 hdfs 블록 사이즈를 넘어설 경우 클라이언트는 namenode에 요청을 하게 된다.
namenode는 파일시스템계층구조에 해당 파일이름을 삽입하고, 이를 위한 데이터 블록을 할당한다.
namenode는 클라이언트의 요청에 datanode와 data block에 대한 정보를 응답한다.
그러면 클라이언트는 로컬 임시 파일에 있는 데이터를 전달받은 정보에 해당하는 datanode에 기록한다.
파일이 닫히게 되는 경우에는 로컬파일에 있는 데이터들이 datanode로 옮겨지고, 클라이언트는 namenode에게 파일이 닫혔다는 사실을 통지하게 된다.

### replication pipelining

hdfs파일의 replication factor가 3이라고 할 떄,
hdfs파일에 클라이언트가 데이터를 write 할 때, 로컬 파일에 블록크기 전체에 해당하는 데이터를 축적했을 경우
클라이언트는 namenode에게 datanode의 리스트를 요청해 받는다.
이 리스트는 해당 데이터블록에 대한 복제본을 가지고 있는 datanode에 대한 정보를 가지고 있다.
클라이언트는 로컬 데이터를 이 리스트의 첫번째 datanode로   옮긴다.
첫 번째 datanode는 이 데이터를 받으면서 자신의 로컬 저장소에 저장하고 이 데이터를 리스트의 두 번째 datanode에 전달한다.
두 번째 datanode는 이 데이터를 받으면서 자신의 로컬 저장소에 저장하고 세 번째 데이터 노드로 옮긴다.
마지막으로 세 번째 노드도 역시 이 데이터를 받으면서 자신의 로컬 저장소에 저장한다.
즉, datanode는 이전 datanode에서 데이터를 받으면서 다음 datanode로 데이터를 옮기는 작업을 수행한다.

### Space Reclamation - file deletes and undeletes

유저 혹은 어플리케이션단에서 파일 삭제요청이 들어왔을 때, 해당 파일이 곧장 hdfs 파일시스템에서 삭제되지 않는다.
우선 hdfs는 요청된 파일을 /trash 디렉토리에 보관한다.
이 디렉토리에 남아 있는 한 파일을 다시 복구 할 수 있다.
/trash 디렉토리에 보관되는 시간은 설정 가능하다(coresite-xml에서 fs.trash.interval, default는 0)
/trash에서 보관시간이 만료되면, namenode는 hdfs namespace에서 해당 파일을 삭제한다.
이어서 실제 파일에 연관된 블록에 있는 데이터도 비워져야 한다.

### replication factor의 감소

파일의 replication factor가 감소하면
namenode는 factor에 비해 초과된 갯수만큼 삭제할 복제본을 선택한다.
선택 후 이 정보를 datanode에게 전달하게 되고, datanode는 해당하는 블록 데이터를 비우게 된다.

## map-reduce

hadoop map-reduce는 많은 양의 데이터를 대용량 클러스터에서 병렬로 처리하는 프레임워크다.

map-reduce Job은 입력 data set을 독립적인 chunk로 나누고 이 chunk들은 map task에 의해 병렬로 처리된다.
이 처리 결과들은 분류 과정을 거쳐 reduce task의 입력으로 전달된다.
![image](https://user-images.githubusercontent.com/33619494/188456133-747d607d-0b47-4999-83a7-75f83fa7f1ab.png)

<span style="color:  #999999;;">그림 3 map-reduce model [출처](https://www.tutorialspoint.com/map_reduce/map_reduce_quick_guide.htm)</span>


### 입출력

map-reduce는 \<key, value> 쌍으로 동작한다.
map-reduce는 job의 입력을 \<key, value>쌍의 set으로 보고 job의 output을 \<key, value> 형태로 반환한다.


### Mapper & Reducer Interface

hadoop에는 Mapper와 Reducer라는 java 인터페이스가 있다.
map-reduce framework의 핵심은 이 인터페이스들이 가지고 있는 map과 reduce 메서드로 동작한다는 것이다.
application에서는 Mapper, Reducer Interface의 map, reduce 메서드를 재정의한다.

\* hive 연동 시 hiveQL이 map-reduce program으로 변환되기에 쿼리 작성만으로 map-reduce를 활용할 수 있다.

### map method

map method에서는 입력 데이터를 독립적으로 처리할 수 있는 별도의 chunk로 분할한다.
입력 값을 intermediate key/value 값으로 매핑 시키는 역할을 수행한다.

intermediate key/value 값 이라는 것이 좀 애매한데
application에서 목표로 하는 job을 수행하기 위해 좀 더 나은 형태로 바뀐 값이라고 생각하면 된다.
hadoop workcount 예제를 예로 들어 보면 이 application에서 목표로 하는 job은 word의 갯수를 세는 것이다.

입력값으로 hadoop hello goodbye hadoop 라는 문자열이 입력되었을 때
wordcount예제의 map method는 다음과 같은 결과값을 반환한다.


```
hadoop/1
hello/1
goodbye/1
hadoop/1
```

### reduce method

map 과정을 거쳐 만들어진 intermediate key/value를 좀 더 작은 단위로 압축시키는 역할을 수행한다.
크게 shuffle(같은 key 값을 가진 데이터 끼리 분류), sort, reduce로 이루어 진다.
wordcount 예제를 예로 들면

위에 있는 mapper 결과물에 대해 아래와 같은 결과값을 반환한다.


```
hadoop 2
hello 1
goodbye 1
```

### 시스템 구성

map-reduce 시스템은 client, job tracker, task tracker구성된다.
![image](https://user-images.githubusercontent.com/33619494/188456369-6a29c95e-3e3a-43e0-acd1-b356b8fed37e.png)

<span style="color:  #999999;;">그림 4 map-reduce 시스템 구성 [출처](https://ryufree.tistory.com/228)</span>

* Client

Job Tracker에게 job을 제출한다.

* Job tracker

namenode에 위치한 소프트웨어 데몬이다.
hadoop 클러스터에 등록된 전체job의 스케줄링을 관리하고 모니터링한다.
task tracker들에게 job을 할당(map과 reduce 태스크)하고, 모니터링 및 work 실패 시 상태 기록을 담당한다.
YARN이 등장하면서 Resource Manager와 Application Master 두 가지 데몬으로 분리 된다.

* Task tracker

datanode에 위치한 소프트웨어 데몬이다.
map-reduce 프로그램을 실행하며, job tracker의 요청을 받아 job tracker가 요청한 map과 reduce 개수만큼 map task와 reduce task 생성
진행 상황 및 결과를 Job Tracker에게 보고한다.
Job Tracker에 의해 제어된다.

## YARN(<span style="color:  #4d5156;;">Yet Another Resource Negotiator</span>)

YARN의 기본적인 아이디어는 Job Tracker의 두 가지 중요한 부분의 책임을 분리하는 것.

Job Tracker --> Resource Manager + Application Manager

즉, 자원 관리를 하는 글로벌  리소스매니저와 애플리케이션별 스케줄링과 모니터링을 하는 애플리케이션 마스터 데몬으로 분리한다.

hadoop 1 버전에서 job tracker는 resource management와 job scheduling/monitoring을 수행했었다.
그리고 하위노드에 해당하는 task tracker에게 map-reduce 작업을 부여하고 주기적으로 그 진행 상황을 보고 받았다.
이러한 구조에서의 문제점은 task 규모가 커짐에 따라 job tracker가 받는 부하가 심해진다는 것이다.
![image](https://user-images.githubusercontent.com/33619494/188456886-5d4bfe73-39a7-4b9c-bf68-07236c6826d4.png)
<span style="color:  #999999;;">그림 5 hadoop v1, v2 Architecture [출처](https://www.edureka.co/blog/hadoop-yarn-tutorial/)</span>

YARN은 resource management 기능과 job scheduling/monitoring 기능을 분리된 데몬에서 처리한다.
YARN의 모든 기능은 3가지 핵심 컴포넌트에 의존한다.

* 리소스매니저
    * 스케줄러와 어플리케이션매니저 두 가지로 나뉘어 진다.
    * 스케줄러 : 플러그인 가능한 스케줄러
    * 어플리케이션매니저 : 사용자 job을 관리, job 제출을 받음
    * 어플리케이션 마스터를 실행하기 위해 Resource Manager와 컨테이너를 협상한다. 또한 어플리케이션마스터 구동을 관리한다.
    * 스케줄러 옵션은 yarn-default.xml에 정의되 있으며 간단히 FIFO, 커패시티, 페어 스케줄러가 있다.

* 노드매니저
    * 각 노드에서 사용자의 job과 워크 플로우를 관리
    * resource manager에 등록 됨.(resource manager를 통해 최신상태를 유지한다)
    * 노드의 상태를 resource manager에 heartbeat으로 전송(노드의 상태와 로그관리를 담당)
    * resource manager로 부터 할당 받은 container를 관리(자원 사용량감시 등)

* 어플리케이션 마스터
    * 사용자 job의 life cycle 관리자
    * 사용자 애플리케이션이 위치하는 곳에 위치

![image](https://user-images.githubusercontent.com/33619494/188457121-3e95667e-3226-4b2a-82c6-04c8ecf524b6.png)
<span style="color:  #999999;;">                                                                        그림 6 hadoop YARN [출처](https://blog.cloudera.com/apache-hadoop-yarn-concepts-and-applications/)</span>

### Resource Manager

자원 분할을 조정하는 역할을 수행한다.
어플리케이션의 자원요구, 스케줄링의 우선순위, 자원의 가용성에 따라 동적으로 자원을 할당한다.

또한 플러그인 할 수 있는 스케줄러 컴포넌트를 가지고 있다.
스케줄러는 애플리케이션의 상태를 모니터링하거나 추적하지 않으며 각종 오류로 인해 태스크가 미 수행 되었을 경우 해당 태스크의 재 시작을 보장하지 않는 순수 스케줄러이다.

resource manager는 모든 클러스터 자원을 감시한다.
클러스터 내의 다양한 경쟁 어플리케이션에 자원을 할당하는 권한이 있다.
이러한 자원 할당의 적용과 추적을 위해 각 노드의 Node Manager와 상호작용한다.

### Application Master

Resource Manager와 자원에 대해 교섭하는 역할을 수행한다.
어플리케이션의 상태 확인과 진행상태를 모니터링 해 스케줄러로부터 적절한 자원 컨테이너의 교섭을 책임진다.
태스크를 실행하고 관리하기 위해 Node Manager와 함께 동작한다.

어플리케이션 마스터는 클러스터의 어플리케이션 실행을 조정하는 프로세스다.
각 어플리케이션은 어플리케이션이 소유한 고유의 어플리케이션마스터가 있다.

어플리케이션 마스터가 기동되면 주기적으로 시스템 상태 확인과 자원 요청의 기록을 갱신하기 위해 heartbeat을 Resource Manager에 주기적으로 전송한다.

### Node Manager

머신 별 slave로써 컨테이너 실행과 자원(CPU, 메모리, 디스크, 네트워크)의 사용률에 대한 모니터링을 담당하며 이를 Resource Manager에게 보고하는 역할을 담당한다.
해당 노드의 사용자 job과 워크 플로우를 관리한다.
Node Manager 기동 시에 Node Manager는 Resource Manager에 등록되고, 자신의 상태를 heartbeat으로 보내고 명령을 기다린다.
Resource Manager에 의해 컨테이너 할당이 이루어지면, 컨테이너 환경을 설정한다.
또한 Resource Manager의 지시에 의해 컨테이너를 종료시킨다.

### 컨테이너

단일 노드에서 RAM, CPU 코어, 디스크 같은 물리적 자원의 집합을 의미한다.
단일 노드에서 여러 컨테이너가 있을 수 있다.
YARN 컨테이너들은 CLC(Container Launch Context)라는 컨테이너 라이프 사이클에 의해 관리된다.

## hadoop HA(High Availability)

hadoop 1버전에서는 namenode가 SPOF(Single Point Of Failure) 상태였다.
이를 보완하기 위한 것이 hadoop HA 구조이다.

전형적인 HA 클러스터는 두 개의 머신을 namenode로 두는데,
특정 시점에 단 하나의 namenode만 active 상태이고 다른 하나는 standby 상태가 되도록 해야 한다.

active namenode는 클라이언트 명령을 처리하고 standby namenode는 active namenode에 발생할 수 있는 장애에 대비를 한다.
standby <-> active 간 상태 동기화를 위해서는 두 노드 모두 journalnode 그룹과 통신을 해야 한다.
jounalnode 그룹이 필요한 이유는 SPOF를 방지하기 위함이다.
hadoop HA는 하둡 버전 1 시절의 SPOF 문제를 해결하기 위해 나왔는데 journal node 가 1개로 구성 되면 journal node 자체가 SPOF가 된다.
따라서 1개로 구성하는 것은 jounal node를 사용하는 의미가 없는 것이고 journal node 클러스터는 홀수로 구성하는 것이 권장되기에 최소 3개 노드로 구성한다.
active node에 의해 namespace modification이 발생하면 active node는 이를 다수의 journalnode들에 기록한다.
standby node는 journal node의 edit log를 모니터링하며 자신의 namespace에 이를 반영한다.
standby node는 failover시 active 상태로 전환되기 전에 journal node의 모든 edit을 반영한 상태가 된다.
빠른 failover을 위해서 standby node가 cluster의 block location에 대한 최신 정보를 가지고 있는 것이 중요한데
이를 위해 datanode들은 namenode들에게 block location과 heartbeat 정보를 보내게 된다.

HA Cluster가 정확하게 동작하기 위해서는 한 순간에 단 하나의 namenode만 active상태여야 한다.
그렇지 않을 경우 namespace 상태의 불일치가 발생할 수 있는데 이를 split-brain senario라고 한다.
journalnode들은 한 순간에 딱 하나의 namenode만이 쓰기를 수행할 수 있도록 제어한다.
failover시 standby 에서 active로 전환되는 node는 이 쓰기 권한을 전달 받는다.

요약하자면
journal node의 역할은 active, standby node의 동기화 그리고 한 순간 하나의 namenode만 write이 가능하게 하는 것.

### Hardware resources

active, standby namenode 두 머신의 spec은 같아야 한다.
journalnode daemon은 상대적으로 가벼워서, namenode와 같은 머신에 구성해 사용할 수도 있다.
단, 최소 3개의 journalnode가 있어야 하며, failover능력을 최대한 높히려면 홀수 개의 journalnode를 구성해야 한다.

또한 기존 hadoop에서는 editLog 파일의 크기를 제한하는 용도로 secondary namednode가 필요했는데
standby namenode는 namespace의 checkpoint역할을 하기 때문에, 따로 secondary node나  checkpointnode, backupnode를 구성할 필요가 없다.
![image](https://user-images.githubusercontent.com/33619494/188457307-a9f005b8-f871-48c5-a012-9e684a5ca63a.png)
<span style="color:  #999999;;">그림 7 jounalnode를 통한 ha 구성 개념도</span>

### Automatic Failover

위에서 구성한 구조는 한 가지 아쉬운 점이 있다.
autumatically하게 active <-> standby 전환이 이루어지지 못한다는 점이다.
아래에서는 automatically하게 failover를 발생 시킬 수 있는 방법에 대해 알아본다.

### Automatic Failover Components

Automatic Failover를 위해서는 hdfs에 두 가지 컴포넌트를 추가적으로 구성해 주어야 한다.

* ZooKeeper

장애 탐지
클러스터의 각 namenode 들은 ZooKeeper에 영속적인 세션을 유지하고 있다.
namenode와 같은 머신에서 구동되는 ZKFC에 의해 세션 유지가 이루어지는데 ZKFC가 NAMENODE의 상태를 주기적으로 체크하면서 namenode가 healthy하다고 판단될 경우 ZOOKEEPER와 세션을 유지한다.
이때 active node에 문제가 생기면  ZooKeeper 있는 세션이 만료되고, standby namenode에게 failover가 필요함을 알린다.

active 노드 선정
active 노드를 선정하는 메커니즘이 있다.

주키퍼 앙상블
주키퍼가 기동되고 있는 서버들의 묶음을 주키퍼 앙상블이라 한다.

주키퍼 앙상블은 홀수로 구성할 것이 권고되는데 그 이유는 다음과 같다.

주키퍼 앙상블이 정상적으로 서비스를 이어가기 위해서는 조건이 필요한데 전체 앙상블을 이루던 서버의 과반수가 정상적으로 작동해야 한다는 것이다.
ex) 4대의 서버로 주키퍼 앙상블을 구성했을 때
4대 중 1대의 주키퍼 서버에 문제가 생긴다면 과반수인 3대의 주키퍼 서버에서 정상적으로 서비스 실행이 가능하므로 문제없다.
하지만 2대의 주키퍼 서버에 문제가 생긴다면 남은 2대의 서버는 과반수 조건을 충족시키지 못하므로 이 경우 주키퍼 서비스가 불가능하다.
즉 4대의 서버로 주키퍼 앙상블을 구성했을 때 허용 결함 수준은 1대이다.

ex) 3대의 서버로 주키퍼 앙상블을 구성했을 때
3대 중 1대의 주키퍼 서버에 문제가 생겨도 과반수인 2대의 서버에서 주키퍼 서비스 실행이 가능하므로 문제없다.
2대의 주키퍼 서버에 문제가 생긴다면 과반수 조건을 충족시키지 못해 주키퍼 서비스가 불가능하다.
3대의 서버로 주키퍼 앙상블을 구성했을 때 허용 결함 수준은 1대이다.

*'결국 4대의 서버로 구성하나 3대의 서버로 구성하나 허용 결함 수준은 1대로 동일하기에 주키퍼 앙상블을 홀수로 구성할 것이 권고된다.'*

* ZKFailoverController process(ZKFC)

namenode를 실행하는 머신은 ZKFC 또한 구동시켜야 한다.
ZKFC가 하는 3가지 역할은 다음과 같다.

Health monitoring
local namenode에 주기적으로 ping을 보내고 그 응답을 받아 local machine의 상태를 감시한다.

ZooKeeper 세션 관리
local namenode가 healthy 상태라면 ZKFC는 ZooKeeper에 session을 유지한다.
또한 local namenode가 active 상태라면 ZKFC는 lock znode를 갖게 된다.

ZooKeeper 기반 선정
local namenode가 healthy하고 lock znode를 가지고 있는 다른 노드가 현재 없다고 보이면, ZKFC는 lock을 획득한다.
lock 획득에 성공하면 ZKFC는 local namenode를 active로 전환한다.