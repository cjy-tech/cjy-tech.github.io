# Elastic Search

ELK에서 Elastic search는 가장 핵심적인 역할을 하고 있습니다.
데이터를 저장하고, 검색하는 코어 엔진을 맡고 있기 때문입니다!
그러나 생각보다 따로 설정을 해야하는 부분은 크게 없다는 것이 참 맘에 드는 녀석입니다.

<span style="color:  #555555;;">제가 Elastic search를 다루면서 중요하다고 생각한 설정은 2 가지가 있고 그 부분만 신경썼었습니다.</span>

<span style="color:  #555555;;">1\. Elastic search의 노드 추가</span>

Elastic search는 기본적으로 모든 설정이 되어있어 크게 건드릴 것이 없었습니다.
설치 후 바로 실행시켜도 문제 없이 ELK에서의 코어 역할을 할 수 있었죠.

그러나! 실행 후에 현재 상태를 확인하기 위해 확인이 가능한 URL에 들어가 보았습니다.
{Elasitc Search 서버 ip} : 9200                             ( 9200 port에 열려있는 Elastic search node의 정보 확인)
![image](https://user-images.githubusercontent.com/33619494/188181235-eddfd1bf-95fa-4499-ab49-fae870951826.png)

{Elasitc Search 서버 ip} : 9200/\_cat/indices        ( 9200 port에 열려 있는 Elastic search의 index상태를 확인)
![image](https://user-images.githubusercontent.com/33619494/188181280-f71742d2-bb6c-43f9-bac0-857b3234e292.png)

각 line 별 항목 : Health, Status, Index, UUID, primary shard, replica shard, docment 수, document 삭제 수, 저장된 size, primary size

위에서 빨간 네모를 친 부분이 Health인데, "이 부분이 왜 Yellow가 되는 것인가" 찾아보았는데요.
![image](https://user-images.githubusercontent.com/33619494/188181310-eaa3acbd-4e2a-416e-9376-cbbe37a9e3c2.png)

Yellow 상태는 Primary shard는 준비가 되었으나, Replica shard가 준비되어 있지 않다! 라는 뜻 입니다.
Replica shard가 없으면 Yellow라는 것은, 필요한 이유가 있기 때문인데 그 것이 무엇일까?

1\. 장애 복구 \(fail\-over\) : 동일한 데이터를 나눠서 가지고 있기 때문에 하나의 노드에 문제가 생겨도\, replica shard가 primary shard로 교체가 됩니다\.

2\. 검색 성능 : replica shard도 읽기 요청을 처리할 수 있기 때문에 replica가 많아질 경우 검색 성능이 증가하지만\, 하드웨어를 더 추가해야만 증가한다고 합니다\.

근데 node가 하나라면 복제를 할 수 없다는 점입니다. 기본적으로 elasticsearch는 같은 노드에 동일한 데이터를 가지는 shard를 저장할 수 없습니다.
![image](https://user-images.githubusercontent.com/33619494/188181383-a3132fd4-380c-4b40-a58f-14c5ea4be97c.png)

이런 식으로 노드를 늘려야 복제 샤드가 할당될 수 있다는 것 입니다. 그러면 이제 어떻게 노드를 증가시키는가!!
간단합니다. elasticsearch를 똑같이 하나 더 설치해서 실행시키면 되니까요 ㅎㅎㅎ
실행하기 전에 각 노드의 설정을 쪼금만 바꾸면 되는데, elasticsearch/config/elasticsearch.yml 파일만 수정하면 됩니다.
<br>
* [Cluster.name](http://cluster.name/) : 클러스터의 이름을 정의한 것으로 <span style="color:  #e11d21;;">노드끼리 같은 값을 가지면 같은 클러스터의 노드로 인식</span>합니다.
* [Node.name](http://node.name/) : 클러스터에서의 각 노드 이름을 설정합니다.
* transport.http.port: http로 전송할 port를 설정합니다. 이는 <span style="color:  #e11d21;;">외부와의 연결할 port</span>를 말합니다. (default : 9200)
* transport.tcp.port: tcp로 전송할 port를 설정합니다. 이는<span style="color:  #e11d21;;">노드와 노드 간 데이터 이동을 위한 port</span>를 말합니다. (default : 9300)
* discovery.zen.ping.unicast.hosts: unicast로 <span style="color:  #e11d21;;">데이터를 주고 받을 host</span>들을 설정합니다. 값으로 나를 제외한 노드의 ip:transport port를 설정합니다.

<span style="color:  #555555;;">discovery.zen.ping.unicast.hosts: ["{데이터 주고 받을 노드 ip}:{tcp.port}, {데이터 주고 받을 노드 ip}:{tcp.port} ...."]</span>

이렇게 설정을 한 후에 키면 노드 간 알아서 master와 slave를 정하게 됩니다.
직접 정하고 싶다면 node.master : true, node.data : false처럼 직접 설정하여 아까 config 파일에 추가하면 가능합니다!

<span style="color:  #555555;;">주의사항 : 저는 다시 설치하기 귀찮아서.... 복사하여 설정만 바꾼 후 돌렸었는데 <span style="color:  #e11d21;;">그럴 경우 elasticsearch/data 폴더를 삭제한 후에 실행</span>시켜야 합니다! data 폴더를 지우지 않을 경우 동일한 데이터를 가지고 있기 때문에 오류가 발생합니다.ㅠㅠ</span>

<span style="color:  #555555;;">2\. ElasticSearch JVM Heap space 크기</span>

ElasticSearch가 Apache Lucene( JAVA 기반 검색 엔진 오픈소스)을 기반으로 되어 있기 때문에 elastic search의 성능에 jvm의 역할이 큽니다.
그 중에서도 Heap Space의 크기가 가장 중요하다고 생각합니다. 메모리를 적게 사용할 경우 elastic search의 성능이 낮아지거나, 메모리 부족으로 뻑이납니다.
<span style="color:  #e11d21;;">Heap 영역을 많이 사용할 경우에 caching에 메모리를 더 많이 쓰게 되기 때문에 성능이 증가하지만...</span>
<span style="color:  #e11d21;;">너무 많으면 garbage collection이 멈출 수 있기에 RAM의 50%가 적당하다고 합니다!</span>
jvm설정을 따로 할 수 있도록 config/jvm.options가 있습니다.  이곳에서 설정을 변경해주면 됩니다!
![image](https://user-images.githubusercontent.com/33619494/188181498-977a2aa0-a027-49ec-879d-1eaea0412b5f.png)

Xms : 최소 heap 메모리 영역 크기
Xmx : 최대 heap 메모리 영역 크기

<span style="color:  #555555;;">자세한 이유는 모르겠지만, elastic search에서 최대와 최소 크기를 동일하게 설정하는 것을 권장한다고 합니다.</span>
<br>
# Logstash

단지 elastic Search만 했을 뿐인데도 너무나도 많네요... 다들 도망가지는 않았을까 걱정이 앞섭니다 ㅠㅠ
그래도 저는 제 갈길 가겠습니다!(강행군) 다음은 Logstash 입니다!

Logstash의 경우에는 패턴 매칭과 파싱을 통해 log를 구조화 시키는 역할을 하게 되는데요. input, filter, output의 구조를 가지고 있습니다.
각 부분에서 많이 사용한 plug-in이나 팁, logstash를 사용할 때 팁들을 공유하고자 합니다\~
자세히하면 너무 길어지기에 참조 위주로 할 예정입니다 ㅎㅎ.

<span style="color:  #555555;;">1\. 데이터를 입력하다\!\! \(input\)</span>

input에서 가장 많이 사용하는 것은 역시나 beats!
그러나 저의 경우에는 beats보다는 stdin(표준입력)과 file을 더 많이 사용했던 것 같네요...
beats로 하기 전에 테스팅을 할 때 stdin과 file을 통해서 했기 때문이죠 ㅎㅎ 제가 많이 사용하던 input을 예시로 올려드립니다!

![image](https://user-images.githubusercontent.com/33619494/188181534-f5096717-f389-43df-aa76-df504e53ac5b.png)
![image](https://user-images.githubusercontent.com/33619494/188181629-d57139b9-42f6-428a-82a9-580a07a7dee5.png)
![image](https://user-images.githubusercontent.com/33619494/188181640-fecde35e-7d43-4f63-8eac-f5c1b9e0f212.png)

* stdin

stdin의 경우에는 아무 설정없이 stdin{}을 하게되면, 한 줄이 하나의 데이터로 인식됩니다. 그래서 여러 줄을 받고 싶을 때 안에 multiline codec을 사용하곤 합니다.
pattern은 multiline을 어느 기준으로 짜르느냐를 설정하고, what은 기준 전까지 짜를지 기준이 나오면 짜를지, negate는 기준에 맞는 애를 자를 것인가, 안맞는 애를 자를것인가...
하하하하하 이렇게 설명하면 이해가 하나도 안되겠죠 저도 그랬으니까요 ㅎㅎㅎ multiline은 나중에 filebeat에서 더 자세히 다루겠습니다\~ (더 읽어달라는...)
<br>
* file

file의 경우에는 path를 설정하면, 해당 파일에 변화가 생길 때마다 인식하여 데이터를 가져옵니다! 자신이 읽었던 곳까지 offset을 설정해놔서, 새로 생긴 데이터만을 가져오게 됩니다.
file도 마찬가지로 한 줄에 하나의 데이터가 default이기에 여러 줄을 받고 싶다면 multiline codec을 사용합니다.
<br>
* beats

beats는 매우 간단합니다! 해당 포트만 설정하면 되기 때문이죠 ㅎㅎ
이외의 설정은 모두 beats에서 하기 때문에 logstash는 데이터를 받기만 하면 됩니다.
나중에 보면 아시겠지만 logstash의 input 설정과 beats의 설정은 매우 비슷합니다. 그래서 logstash로 먼저 test를 해보고, 그 설정을 beats에 적용하는 방법을 추천드립니다!

<span style="color:  #555555;;">나머지 plug-in은 [https://www.elastic.co/guide/en/logstash/current/input-plugins.html](https://www.elastic.co/guide/en/logstash/current/output-plugins.html) 를 참고해주세요.</span>

<span style="color:  #555555;;">2\. 내가 원하는 대로 만들어보자\~ \(filter\)</span>

대망(大亡)의 filter에 도착했네요 ㅎㅎ
사용자가 원하는 대로 log를 구조화시킬 수 있다고는 하지만! 거짓말이에요 원하는 대로 만들기 너무 어렵습니다...
그래도 구조화시키는데 제가 생각했을 때 그나마 편하고, 도움이 많이 되었던 것들을 소개하고자 합니다!

* Grok 패턴

grok 패턴은 들어온 메세지를 정규표현식을 통해 field와 value로 지정하는 방법입니다.  예시를 통해보면 더 빨리 이해가 되리라 믿습니다!
grok 패턴 기본 구조 : grok { match => {"적용할 field명" => "매칭할 정규표현식"} }
![image](https://user-images.githubusercontent.com/33619494/188181770-903c917a-b12e-447f-9818-792c78190ba2.png)

![image](https://user-images.githubusercontent.com/33619494/188181730-808e0378-017e-4447-834e-a083c10a7e40.png)


초록 박스를 만나게 되면 grok 패턴이 매칭되어,
{date = 01/10/18} {time = 15:43:46.372} {type = LOCK\_TIMEOUT}과 같이 위의 패턴에서 설정한 대로 저장되는 것을 확인할 수 있습니다.
빨간 박스는 매칭이 불가능한 log 형태이므로, grok 패턴을 적용시킬 때 첫번째 줄만 적용시키도록 해야합니다!
![image](https://user-images.githubusercontent.com/33619494/188181809-2445d666-5408-41c4-b170-eb055a8612d9.png)



정규표현식의 고수가 된다면, 나중에 grok 패턴을 이런 식으로 사용할 수 있다고 합니다...

제가 grok 패턴을 사용하면서 유용하게 사용했던 사이트들을 공유드립니다.

<span style="color:  #555555;;">1\. grok 패턴 테스트 사이트 [http://grokconstructor.appspot.com/do/match](http://grokconstructor.appspot.com/do/match)</span>

<span style="color:  #555555;;">작성한 패턴이 제대로 작동되는 지 확인할 수 있는 사이트 입니다. 내가 세운 패턴을 입력하고, 적용할 테스트 케이스를 입력하여 돌리면,어떤 field에 어떤 값이 저장되는지 확인할 수 있습니다.</span>

<span style="color:  #555555;;">2\. 사전 정의된 정규표현식 패턴 [https://github.com/elastic/logstash/blob/v1.4.2/patterns/grok-patterns](https://github.com/elastic/logstash/blob/v1.4.2/patterns/grok-patterns)</span>

사람들이 자주 사용할 것같은 패턴을 미리 정의해 놓은 것들을 보여주는 사이트 입니다.
날짜, 시간, ip주소 와 같이 사용이 빈번한 것들은 우리가 직접 정규표현식을 짤 필요없이, 여기 나온 변수명을 사용합시다!

* Kv 패턴

kv 패턴도 매우 유용하게 쓰곤 했습니다! 특히 field : value 형식과 같은 log에서 매우 유용하게 쓰이는데요.
제가 사용했던 예시를 보면 더 확실히 느껴지실 것입니다
![image](https://user-images.githubusercontent.com/33619494/188181885-4f49e05b-e16f-40fd-a81d-7b64cd849131.png)

여기서 source 는 패턴을 적용할 field명입니다.
field_split은 field와 value 쌍을 나누는 기준! value는 field와 value 쌍으로 나눈 것 중에 어떤 것을 기준으로 field와 value로 나눌 것인가!
이게 무슨소리야.... 이럴 땐 예시가 직빵이죠 ㅎㅎ
![image](https://user-images.githubusercontent.com/33619494/188181945-9e713a51-b71e-4b3b-bff7-e71ac96d6311.png)

위의 설정을 가지고 적용한다면!
"\n" (줄바꿈) 을 기준으로 하나의 field + value 를 나눈다. -> field + value에서 둘을 나누는 기준은 ":"이다.
![image](https://user-images.githubusercontent.com/33619494/188181967-936b243e-3fc2-4eec-b9d8-b992d107570f.png)

적용 후의 결과 입니다. 확인해보시면, 각 줄 별로 ":"을 기준으로 좌측은 field가 되고, 우측은 value가 된 것을 확인할 수 있습니다.
매우 유용한 패턴으로, 자세히 찾아보면 더 좋다는 것을 알 수 있습니다~

* Ruby

logstash의 filter에서는 Ruby 코드를 이용하여 파싱이 가능합니다.
저도 ruby를 많이 사용했는데요. 주로 날짜 형식을 맞추기 위해서나, 배열을 사용하거나, 끝의 글자를 몇 개 자르는 것과 같이 logstash가 기본적으로
제공하는 filter로는 하기 힘든 작업들을 수행하곤 했습니다.
Ruby도 여기 와서 처음 다뤄봤으나, 큰 프로그램을 짜는 것이 아니라서 생각보다 도큐먼트보면서 하면 할만 했습니다.

Ruby 도큐먼트 : [https://ruby-doc.org/stdlib-2.3.1/libdoc/](https://ruby-doc.org/stdlib-2.3.1/libdoc/)

나머지 기본적인 filter들은 elastic 공식 사이트를 참고하시면 될 것 같습니다!
[https://www.elastic.co/guide/en/logstash/current/filter-plugins.html](https://www.elastic.co/guide/en/logstash/current/filter-plugins.html)

3\. 구조화된 데이터를 내보내자\! \(output\)

가장 많이 사용하는 output은 역시나 elastic search!
그리나 테스트할 때 표준 출력인 stdout을 많이 써서 stdout을 가장 많이쓴거 같네요.
<br>
* elastic search

elastic search의 설정은 매우 간단합니다. port와 index 명만 설정해주면 되기 때문이죠 ㅎㅎㅎ
여기서 index 명을 설정할 때 tip!
![image](https://user-images.githubusercontent.com/33619494/188182001-bb9bbe84-8324-40c2-8b5c-d009a05d1279.png)

index는 관리하기 편한 포맷으로 설정하는 것이 좋습니다. 그래서 저는 db_(어떤 로그인지)_(날짜) 형식으로 저장을 했는데요.
위에서 확인할 수 있듯이 %{field명}을 통해 변수처럼 이용할 수 있답니다!

* stdout

표준 출력 stdout은 매우 쉽습니다, 그냥 stdout{}을 하면 끝나니까요.
쪼금 이쁘게 보고 싶다! 하면 codec을 추가하시면 됩니다. 저는 주로 rubydebug를 추가하여 사용했습니다 \~

<span style="color:  #555555;;">4\. logstash 팁\!</span>

1\. logstash는 기본적으로 실행시킬 때 conf 파일이나 conf 파일이 담긴 폴더을 지정하면서 실행할 수 있습니다\.
bin/logstash -f {conf 파일}, bin/logstash -f {폴더}
폴더에서 여러 개의 conf파일을 beats로 받을 경우 beats의 port를 다르게 하는 게 좋습니다.
<span style="color:  #e11d21;;">하나의 데이터가 들어왔을 때 폴더 내의 모든 conf 파일을 거치기 때문에 조심해야합니다!</span>

2\. elastic search와 마찬가지로 heap 영역의 size를 정할 수 있습니다\.
jvm.option에서 elastic search와 동일하게 변경 가능합니다!

<span style="color:  #555555;;">3\. 하나의 서버에서 2개 이상의 logstash instance를 돌릴 수 있습니다\.</span>
<span style="color:  #555555;;">그냥 실행시키면 데이터 폴더가 이미 사용 중이라서 실행이 불가하다고 오류 메세지가 뜨게 됩니다.</span>
<span style="color:  #555555;;">그럴 경우 </span>**<span style="color:  #ff0000;;">데이터 폴더를 직접 설정</span>**<span style="color:  #555555;;">해주는 방법이 있습니다.</span>
<span style="color:  #555555;;">bin/logstash -f {conf 파일} --path.data {데이터 저장 폴더}</span>
<span style="color:  #555555;;">이렇게 실행할 경우 문제 없이 2개의 instance를 돌릴 수 있습니다!</span>
<br>
# Kibana

Kibana는 사실 미적 감각만 있다면 크게 고민할 부분이 없습니다.
그래도 몇 가지 팁과 조심해야할 점을 공유하고 합니다!

<span style="color:  #555555;;">1\. kibana 기본 설정 시 조심해야할 점</span>

kibana는 기본적으로 설정할게 많지 않지만 딱 한가지 조심해야할 점이 있습니다.
바로 config/kibana.yml 에서 server.host!!
처음에는 당연히 localhost나 127.0.0.1으로 해도 되겠지!!했다가 실패했습니다...
그 이유는 kibana는 client에서 작동하는 것으로
<span style="color:  #e11d21;;">server host로 설정된 ip를 client PC가 접근을 하는 것입니다!!</span>
그렇기 때문에 외부 ip로 설정을 줘야합니다. localhost로 하면 client의 localhost로 접근하는 꼴이 되어버리는 것이죠ㅠㅠ

<span style="color:  #555555;;">2\. 차트에서 날짜 설정</span>

제가 처음에 애를 먹었던 부분인데요,,,,
차트에서 날짜를 내 마음대로 설정하고 싶었는데, 날짜의 범위를 주는 방식이 익숙하지 않아서 힘들었습니다.

![image](https://user-images.githubusercontent.com/33619494/188182042-78eb9565-dc96-4ba7-b5d8-87f3f3c46a6a.png)
![image](https://user-images.githubusercontent.com/33619494/188182181-f85f23f8-2407-494e-9d68-c6ed0c4b34bd.png)

위의 방식과 같이 범위를 설정하게 됩니다.
좌측은 오늘을 기준으로 7일 전까지의 데이터를 확인할 수 있는 설정입니다.
우측은 이번 주 월요일을 기준으로 7일 전까지의 데이터를 확인할 수 있는 설정입니다.

<span style="color:  #e11d21;;">/d를 하는 경우에는 시작점이 그 날 오전 9시가 기준이 됩니다.</span>

<span style="color:  #e11d21;;">/w를 하는 경우에는 시작점이 그 주 월요일 오전 9시가 기준이 됩니다.</span>

3\. 생각보다 유용한 visualize

* Data Table

데이터 테이블은 원하는 정보만을 뽑아서 표 형태로 보여주는 매우 좋은 녀석입니다ㅎㅎ
![image](https://user-images.githubusercontent.com/33619494/188182247-28d527a1-9f15-49d8-acde-9a1ad8971bb5.png)

위의 화면처럼 원하는 데이터를 한눈에 볼 수 있게 구성할 수 있어서 로그를 확인할 때 가장 유용하게 쓰입니다.
<br>
* Markdown

Markdown은 처음에는 사용하지 않았지만 나중에 대시보드 구성할 때 각 차트를 설명해놓는 용도로 쓰이게 되었습니다.
혼자 쓰시는 분들은 크게 필요가 없겠지만.... 혼자쓰기엔 너무 아깝잖아요 ㅎㅎㅎ
<br>
* Controls

controls은 검색을 통해 원하는 데이터만을 필터링해주는 역할을 합니다.
원하는 로그 정보를 검색하여 찾고 싶은 경우 매우 편리합니다.
<br>
# Filebeats

드디어 마지막 filebeats 입니다!
다들 고생하셨고, filebeats는 유용한 config를 공유해드리고 마치겠습니다 ㅎㅎ

<span style="color:  #555555;;">1\. 한번에 여러 줄을 가져오는 multiline</span>

로그들 대부분이 1줄짜리가 없습니다. 한번 로그가 떨어지면 여러 줄이 떨어지는 것이 기본인데,
filebeats는 기본적으로 1줄씩 메세지를 담아 보내는 특성이 있습니다. 그래서 multiline을 통해 여러 줄을 하나의 메세지로 보내는 것이 중요한데...
중요한 만큼 처음에 어렵더군요.... 속성도 매우 다양합니다.
<br>
* Multiline.max\_line : 하나의 메세지의 최대 길이를 설정합니다.
* Multiline.pattern : 하나의 메세지로 인지하는 패턴, 여기까지가 하나의 메세지다!를 결정하는 기준이 됩니다.              multiline의 패턴을 설정하는 부분은 정규표현식을 기반이지만, 조금 다른 형태를 가집니다.  [https://www.elastic.co/guide/en/beats/filebeat/current/regexp-support.html](https://www.elastic.co/guide/en/beats/filebeat/current/regexp-support.html) 홈페이지를 참조해주세요!
* Multiline.negate : 패턴과 매칭된 애를 기준으로 자를까, 패턴에 맞지 않는 애를 기준으로 자를까를 결정합니다.
* Multiline.match : 위에서 결정된 애를 앞에 붙일 것인가, 뒤에 붙일 것인가......

말로 설명하니까 이게 무슨 소리야.... elastic 사에서도 그럴 줄 알고 참고 사진을 올렸더군요 ㅎㅎ
![image](https://user-images.githubusercontent.com/33619494/188182294-73e99590-810c-451d-8f49-4b51681fbf47.png)

공식 사이트에 있는 그림을 참조하여, 이해하시면 편할 것입니다!
저는 개인적으로, pattern을 설정해서 negate: true, match : after를 주로 사용했습니다.
패턴에 맞는 애들 기준으로 뒤에 붙이는 방식으로요!! 가장 직관적인 방법이라고 생각되네요 ㅎㅎ

<span style="color:  #555555;;">2\. 정보를 추가로 넣어주자\! Tag\, Field</span>

Tag와 Field를 통해 filebeats에서 정보를 추가하여 보낼 수 있습니다.
저의 경우에도 이 로그가 slow query인지, lock 인지 알기 위해 tag에 추가하여 보내기도 했습니다.
Field를 이용하여  mysql의 경우에 port정보도 중요한데, 이는 로그에 존재하지 않아서 추가하여 보내는 방식을 택했습니다!

<span style="color:  #555555;;">3\. 한번에 너무 많이는 부담스럽자나\.\.\.\. ignore older</span>

filebeats의 경우에 자신이 읽었었던 부분을 offset을 설정해 data/registry 파일에 저장해놓습니다.
정보를 읽는 작업을 haverst라고 하는데, registry 파일을 참조하여 어느 파일의 어느 부분을 수확할지 정합니다.
이러한 과정이 있기에 평소에는 큰 부하가 없습니다.
그러나 filebeats가 오랫동안 죽었다가 살아나거나, 첫 배포를 할 경우에 너무 많은 로그를 끌어올 수 있기 때문에  방지책이 필요했습니다.
ignore older! 는 수확할 파일 중에 수정시간을 참조하여, 설정한 시간보다 오래된 파일은 무시하게 됩니다.
1m 이면 1분, 1h이면 1시간 이상 수정이 일어나지 않은 파일은 무시하게 됩니다