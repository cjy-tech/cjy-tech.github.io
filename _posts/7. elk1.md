# ELK( Elastic Search, Logstash, Kibana)란 무엇인가

<span style="color:  #555555;;">'ELK( Elastic Search, Logstash, Kibana) Stack이란 무엇인가' 를 먼저 설명해드려야할 것 같습니다.</span>

ELK는 로그들을 수집하여 실시간으로 분석하고 시각화를 해줄 수 있는 모듈들의 집합으로, 현재는 Beats라는 모듈도 많이들 추가하여 사용하고 있습니다.
각 모듈이 어떠한 일을 수행하는 지 간단히 설명해보겠습니다

1\. ElasticSearch

Apache Lucene(JAVA로 작성된 검색 엔진 오픈소스)을 바탕으로 개발한 분산 검색엔진으로 비정형(schema-less) 데이터를 쉽게 저장하고 처리할 수 있으며, 
실시간 검색과 플러그인을 이용한 확장을 지원하는 등의 장점을 가지고 있습니다. 또한 인덱스 갱신 주기도 빠른 편이며, 운영중 스키마 변경이 가능합니다.
ELK에서는 받아온 데이터를 저장하고, 저장한 데이터를 조회해주고, filter를 설정하여 보고싶은 데이터만을 추출해주는 역할을 담당하고 있습니다.
Elastic Search가 ELK의 중심에 있고, 가장 핵심적인 역할을 수행하고 있습니다!

2.Logstash

Logstash는 일정한 규칙에 맞춰서 떨어지는 log를 구조화시켜주는 역할, 즉 들어오는 데이터를 가공하여 Elastic Search에 보내는 ETL(Extract, Transform, Load)역할을 담당하고 있습니다.
input - filter -output의 구조로 되어 있습니다. 입력을 받은 데이터를 filter에서 패턴 매칭, 파싱 작업을 통해 데이터를 구조화시키고 구조화된 데이터를 Elastic Search로 보내는 역할을 하고 있는 것 입니다.
핵심 엔진은 Elastic Search이지만 가장 애를 먹었던 녀석은 Logstash 였습니다. 로그의 형태에 맞게 패턴 매칭과 파싱을 직접 구현해줘야하기 때문에 정규표현식, ruby에 대한 지식도 필요했기 때문입니다. ㅠㅠ

3\. Kibana

Kibana는 웹 기반 데이터 분석 및 대시보드 작성 툴을 제공하여 데이터를 시각화하는 역할을 담당합니다. Elastic Search의 결과를 그래프나 차트를 이용해서 보여주는 역할을 합니다.
저의 경우에는 kibana를 다루는 것도 어려웠습니다 ㅠㅠ 미적 감각을 필요로 하거든요..ㅎㅎㅎ
그리고 이건 오픈소스의 단점이라고 볼 수 있는데, 화면 구성에 제한되는 부분이 많습니다. 제가 원하는 대로 구성하기 힘든 부분들이 존재했습니다.

4\. Beats

Beats는 로그 수집을 원하는 서버에 에이전트로 설치하여, 데이터를 Elastic Search에 또는 Logstash에 특별한 가공없이 보내는 역할을 하는 데이터 발송자입니다. 데이터를 수집하는 데에 있어 logstash는 서버에 부하가 크기 때문에 beats라는 가벼운 에이전트로 대체하여 DB 서버에 부하를 줄인 것 입니다.
추적하고자하는 파일을 설정하면, 그 파일이 생성되거나 수정될 경우에 이를 추적하여 새로운 데이터들을 보내주는 역할을 합니다.

# ELK 구조
![image](https://user-images.githubusercontent.com/33619494/188178345-e2898bf0-aec6-4c91-9e5b-fa511dbe377a.png)

제가 프로젝트를 진행하면서 구현한 결과물의 구조이고, ELK를 사용하는 일반적인 구조입니다.
일반적 ELK 구조에서 제가 Elastalert라는 모듈을 하나 추가하여 구성해봤습니다.

<span style="color:  #555555;;">Elastalert는 python으로 개발된 오픈소스인데, 추적하는 로그가 들어왔을 때 알림을 주는 역할을 합니다.</span>

<span style="color:  #555555;;">특별히 예의 주시하는 DB에서 Lock이 발생했거나, Lock이 걸린 시간이 몇 초 이상인 로그처럼 바로 알아야하는 에러가 발생 했을 때 메일이나, slack(우리 회사는 dooray)로 메세지를 발송합니다.</span>

1\. 각 DB 서버에 존재하는 Beats가 사용자가 원하는 log파일을 추적하고\, 변화가 생기면 Logstash에 변경된 사항을 보내게 됩니다\.

2\. Logstash에서는 Beats가 보내 준 데이터를 패턴 매칭\, 파싱을 통해 구조화를 시킵니다\.

3\. 구조화된 데이터를 Elastic Search에 보냅니다\.

4\. Kibana가 Elastic Search에 있는 데이터를 가지고 시각화를 합니다\.

# 수행과정

위에서 본 구조를 보면 크게 와닿지 않을 것이라 생각되어!
제가 진행했던 프로젝트에서 어떤 로그를 수집했고\~ 어떤 식으로 로그를 패턴 매칭을 했는지 보여드리면서 설명하고자합니다.

<span style="color:  #555555;;">수집 로그 및 수집 요구사항</span>

<span style="color:  #555555;;">CUBRID</span>

| 로그 | 요구사항 |
| :---: | :---: |
| Lock | 발생시간, host, DB명, Lock type, SQL문, binding 객체, 메세지 전체 |
| Slow query | 발생시간, host, DB명, Broker명, Query 문, binding 객체, 수행시간, 메세지 전체 |
| Server Error | 발생시간, host, DB명, Error type, Error 코드, Error 메세지, 메세지 전체 |
| Broker Error | 발생시간, host, DB명, Broker명, Error type, Error 코드, Error 메세지, 메세지 전체 |

<span style="color:  #555555;;">MYSQL</span>

| 로그 | 요구사항 |
| :---: | :---: |
| Slow | 발생시간, host, 수행시간, 사용자, SQL문, Lock time, Scanned 수, Result 수 |

위와 같이 각 수집하고자 하는 로그가 있고, 로그마다 구조화하고자 하는 데이터들을 뽑아냈다면!
어디서 로그를 가져오고, 그 로그를 분석하여 어떤 식으로 패턴을 세우고, 파싱을 하는지 결정을 해야합니다.
이해를 돕기 위해 제가 했던 로그들 중 대표적인 사례 한 가지만 올려보려 합니다.

<span style="color:  #555555;;">CUBRID Lock log</span>

<span style="color:  #555555;;">1\. 어디서 로그를 가져오는가\! \(Filebeats\)</span>

로그를 가져오기 위해서는 filebeats 에이전트를 해당 서버에 배포를 한 후에, filebeat 내에서 어느 경로에 있는 파일을 추적할 것인가를 설정해야합니다. 저와 같이 대량으로 배포해야하는 경우에는 ansible이라는 DevOps 툴을 사용하는 것을 추천 드립니다.

![image](https://user-images.githubusercontent.com/33619494/188178491-8a8ab783-78af-4912-b5ea-fd6938972a45.png)

위의 화면은 filebeat에서 추적할 로그 파일을 설정해주는 yml 파일의 내용입니다.
path부분에 로그 파일의 경로를 작성합니다! (cubrid의 경우에 rotation 때문에 추적해야할 파일이 많아 wildcard(*)를 사용했습니다.)
multiline은 filebeat는 기본적으로 1줄씩 가져오게 되는데, 원하는 로그가 1줄이 넘어가는 경우 설정합니다!
ignore_older 속성은 filebeat를 처음 배포하게되면 추적하는 파일의 양이 엄청나게 됩니다... 그래서 "설정한 시간보다 더 오래 전에 수정된 파일은 추적하지 않겠다"라고 설정한 부분입니다.
이렇게 Beats는 떨어지는 로그를 아무런 가공없이 logstash 서버에 보내게 됩니다!

<span style="color:  #555555;;">2\. 로그를 분석하고 구조화 시키자\! \(Logstash\)</span>
![image](https://user-images.githubusercontent.com/33619494/188178588-e8daa7bb-6921-4252-94ac-82dcb5bac677.png)


Cubird DB에서 떨어지는 Lock 로그 입니다! 주요한 정보는 보라색로 밑줄 그은 시간, 빨간색의 type, 초록색의 SQL문 및 bind 객체입니다.
위의 로그를 보고, 첫번째 줄에서 시간을 추출하고, '-'(하이픈) 뒤에서 type을 추출하고, "sql :"을 찾아서 뒤에 내용을 SQL문으로추출하고, "bind :" 를 찾아서 binding 객체를 추출하자!!
그러나 위의 사례들은 굉장히 양호한 상태의 Lock이었죠.....
![image](https://user-images.githubusercontent.com/33619494/188178634-2d45a1a0-01a5-4206-99ba-a10b9f114006.png)



이야아..... 말문이 막히더라구요... 이게 뭔가 싶었죠....
하지만 Logstash에서 제공하는 패턴들(kv 패턴, grok 패턴)과 ruby 코드를 이용해서 원하는 데이터들을 추출할 수 있었습니다.
자세한 내용은 여기서 다루기에 너무 복잡하고, 읽기 싫은 스크롤이 되어버릴까봐... 나중에 따로 logstash와 filebeat의 설정에 대해서 올릴까합니다 ㅎㅎ

![image](https://user-images.githubusercontent.com/33619494/188178678-a634d9bb-91fe-49b6-a5ff-b855e5b1d576.png)

![image](https://user-images.githubusercontent.com/33619494/188178712-92478b8a-85fd-4d85-8400-622ffdc40285.png)


결국 이렇게 하나의 Lock에 걸린 sql문들을 배열 형태로 저장하고, 그에 따른 binding 객체도 배열 형태로 얻을 수 있게 되었습니다 ㅠㅠ

각 로그에 맞춰 이런 패턴 매칭과 파싱 작업이 끝나면, 이 정보를 elastic search 보내게 됩니다.
위의 데이터들을 Elastic search에 보내는 데,  데이터는 각 로그마다 다른 index( RDBMS에서 말하는 table) 저장하는 것이 이후에 분류하기 좋습니다.
저의 경우에는 관리하기 쉬운 형태로 index명을 설정하여 관리하였습니다.
![image](https://user-images.githubusercontent.com/33619494/188178746-47fd93b1-12e9-4952-9684-2f9eb09cbc70.png)


위의 그림에서 빨간 박스가 제가 저장한 index의 포맷으로, "db_????_날짜"로 정규표현식으로 규정하기도 편하고, 날짜별로 관리하기도 편한 포맷을 선택했습니다.

<span style="color:  #555555;;">3\. 로그의 정보를 보기 좋게 시각화 하자\! \(Kibana\)</span>

이제 저장한 데이터를 시각화하는 일만 남았습니다~
아까 위에서 index를 관리하기 편한 포맷을 선택한 이유는 kibana에서 알 수 있습니다.
kibana는 시각화를 원하는 index들을 묶어서 관리할 수 있도록 index pattern을 생성합니다. 이때 분류하기 편하게하기 위해서!! 저런 포맷을 선택하게 된 것입니다.

![image](https://user-images.githubusercontent.com/33619494/188178817-c06ae529-ca5c-4337-9d9e-c10e40681a39.png)

![image](https://user-images.githubusercontent.com/33619494/188178855-b96cf7e3-25a5-4521-a9fa-a6e89ba354ca.png)

위의 그림에서 볼 수 있듯이 제가 차트, 그래프를 생성하거나 데이터를 보기 위해서는 모두 index pattern을 이용하게 됩니다 ㅎㅎ
그래서 이제는 꾸미는 일만 남았는데요. 꾸미기를 원하는 index pattern을 선택하여 그래프를 그리기만 하면 됩니다.
kibana는 작업이 생각보다 쉽습니다.
그러나!! 쉽기 위해서는 데이터를 어떻게 시각화할지 정해놓고, 데이터를 구조화를 해야한다는 것을 명심해야 합니다.(저는 kibana 작업을 하면서 logstash 패턴과 파싱을 몇 번씩 수정했습니다ㅠㅠ)

이러한 과정을 거치고나면 시각화한 데이터들을 하나의 대시보드에 구성할 수 있습니다. 제 결과물은 이렇습니다!

![image](https://user-images.githubusercontent.com/33619494/188178915-e1b6fe29-e701-4d2e-a335-94cc945e25ac.png)
![image](https://user-images.githubusercontent.com/33619494/188178946-a53ba13a-f3c8-4b6d-a1db-bcb435d56ad0.png)
![image](https://user-images.githubusercontent.com/33619494/188178965-0a41c1b5-2a28-46bc-be33-7b71598a402a.png)
![image](https://user-images.githubusercontent.com/33619494/188178978-348bb591-2610-44af-8d72-a02a22ca2583.png)
