# 설치에 앞서

* 설치 장비 수 4대
* 장비 환경
    * Centos 7.4.0
* 최종 설치 구성도는 아래와 같다.
![image](https://user-images.githubusercontent.com/33619494/188458808-0b4e67c3-9784-4e1e-8252-83d8dcd1ea38.png)

# 설치

## jdk 8 설치

### java download

* 4개 서버의 /usr/java 경로에 설치한다.

```
wget [https://download.java.net/openjdk/jdk8u41/ri/openjdk-8u41-b04-linux-x64-14\_jan\_2020.tar.gz](https://download.java.net/openjdk/jdk8u41/ri/openjdk-8u41-b04-linux-x64-14_jan_2020.tar.gz)
tar xvzf openjdk-8u41-b04-linux-x64-14_jan_2020.tar.gz
```

### java path 관련 설정

* JAVA\_HOME을 모든 계정에 적용시키기 위해 JAVA\_HOME을 /etc/profile에 설정

/etc/profile에 아래 내용 추가한다.

```
export JAVA_HOME=/usr/java/java-se-8u41-ri
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/lib/tools.jar
```

저장한 후 아래 명령어를 실행한다.

```
source /etc/profile (수정사항 반영)
java -version
javac -version (설정 확인)
```

## ZooKeeper 3.5.9 설치

### ZooKeeper download

* 서버 2,3,4에 ZooKeeper를 설치한다.
* 주키퍼는 홀수로 설치해야 하며 최소 3개 노드에 설치해야 하기에 서버 2, 3, 4에 설치한다.
* /home1/irteam 경로에 다운로드 받는다.
* wget certificate 오류가 있어서 --no-check-certificate 옵션을 추가했다.

```
sudo wget --no-check-certificate [https://dlcdn.apache.org/zookeeper/zookeeper-3.5.9/apache-zookeeper-3.5.9-bin.tar.gz](https://dlcdn.apache.org/zookeeper/zookeeper-3.5.9/apache-zookeeper-3.5.9-bin.tar.gz)
sudo tar xzvf apache-zookeeper-3.5.9-bin.tar.gz
```

### zoo.cfg 파일 설정

* 주키퍼 설정파일을 지정해주기 위해 아래 경로로 이동한다.

```
cd /home1/irteam/apache-zookeeper-3.5.9-bin/conf
```

* 아래 명령어를 실행한다.

```
cp zoo_sample.cfg zoo.cfg (주키퍼 서버 기동 시 기본적으로 zoo.cfg를 참조해 실행됨)
```

* zoo.cfg 를 열어 아래 내용을 설정해준다. 서버 2, 3, 4에 공통으로 설정한다.
    * dataDir의 경로는 없을 시 직접 생성해 주어야 한다.

```
tickTime=2000 
initLimit=10 
syncLimit=5 
dataDir=/var/lib/zookeeper/
clientPort=2181
server.1=해당서버ip:2888:3888 
server.2=해당서버ip:2888:3888
server.3=해당서버ip:2888:3888
```

* 설정 값 설명
    * tickTime: 하나의 tick 당 몇 millisecond인지, 2000이면 1tick 당 2000milliseconds
    * initLimit: 주키퍼 서버가 leader 서버에 연결하는 것에 대한 제한 시간이고 단위는 tick. 5 라고 적혀있으면 5 \* tickTime = 5 \* 2000milliseconds= 10초
    * syncLimit: 서버가 leader 서버로부터 동기화 되지 않을 수 있는 최대 마지노선 시간. 단위는 tick
    * server list : 주키퍼 앙상블을 이루고 잇는 서버들이 서로를 알 수 있게 하기 위함
        * server.id의 형식에서 id는 1, 2, 3을 사용하였고 이를 각각 알파 장비 server 2, 3, 4에 매핑 시켰다.
        * port 2개 중 첫 번째 port: Quorum port
        * ZooKeeper leader election 용 포트

### myid 파일 생성 및 설정

* 각 서버의 /var/lib/zookeeper로 이동(위의 zoo.cfg 설정한 dataDir 항목의 값)하여 myid라는 파일을 생성한다.
* 파일 안에 위의 server list에서 설정한 서버 id를 기입한다.
    * server 2, 3, 4에 대해 1, 2, 3이라는 id를 사용하기로 했으므로 각 서버의 myid 파일에 1, 2, 3의 숫자를 각각 설정한다.

### initialize 디렉토리 생성

* 위와 같은 경로에 (/var/lib/zookeeper) initialize라는 이름의 디렉토리를 생성한다.

### ZooKeeper 서버 기동 및 동작 확인

* bin/zkServer.sh start로 server 2, 3, 4의 주키퍼를 기동시킨다.
* ./zkServer.sh status로 확인한다.
    * 3개의 서버 중 1개는 leader로, 나머지 2개는 follower로 동작함을 확인 할 수 있다.
    * leader 서버 shutdown 시 follower 서버 중에서 leader 선출이 새롭게 이루어짐도 확인 할 수 있다.

### Automatic failover 관련 설정

* ZKFC 프로세스가 기동되어야 하는 namenode서버(server 1, 2)의 hdfs-site.xml에 아래 내용을 추가한다.
    * 이 설정을 해 두면 [start-dfs.sh](http://start-dfs.sh/)실행 시 내부 코드에서 zkfc를 start 시킨다.

```
<property>
   <name>dfs.ha.automatic-failover.enabled</name>
   <value>true</value>
 </property>
```

* server 1, 2의 core-site.xml에 아래 내용을 추가한다.
    * 설정 되어있지 않으면 ./hdfs zkfc -formatZK시에 에러 발생

```
        <property>
                <name>ha.zookeeper.quorum</name>
                <value>gbia-hdpalp-wb802:2181,gbia-hdpalp-wb803:2181,gbia-hdpalp-wb804:2181</value>
        </property>
```

## hadoop 2.6.0 설치

### hadoop download

* server1, 2, 3, 4의 /home1/irteam 경로에 hadoop 2.6.0을 다운로드 받는다.

```
wget [https://archive.apache.org/dist/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz](https://archive.apache.org/dist/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz)
sudo tar xvzf hadoop-2.6.0.tar.gz
```

### ha 구성 관련 설정

* ha일 때의 hdfs 경로 설정
* server 1, 2, 3, 4에 hdfs-site.xml에 아래 내용을 추가한다.

```
        <property>
                <name>dfs.nameservices</name>
                <value>alpha-dev</value>
        </property>

        <property>
                <name>dfs.ha.namenodes.alpha-dev</name>
                <value>namenode01, namenode02</value>
        </property>


        <property>
                <name>dfs.namenode.rpc-address.alpha-dev.namenode01</name>
                <value>gbia-hdpalp-wb801:8020</value>
        </property>
        <property>
                <name>dfs.namenode.rpc-address.alpha-dev.namenode02</name>
                <value>gbia-hdpalp-wb802:8020</value>
        </property>
```

* http 서버 설정
* namenode(active 및 standby)로 사용할 server 1, 2의 hdfs-site.xml에 아래 내용을 추가한다.
    * server 1, 2 중 하나는 active 하나는 standby 상태

```
        <property>
                <name>dfs.namenode.http-address.alpha-dev.namenode01</name>
                <value>gbia-hdpalp-wb801:50070</value>
        </property>
        <property>
                <name>dfs.namenode.http-address.alpha-dev.namenode02</name>
                <value>gbia-hdpalp-wb802:50070</value>
        </property>
```

* journal node url 기입
    * server 1, 3, 4는 journal node 프로세스가 수행됨
    * namenode 들은 journal node 들의 위치 정보를 알고 있어야 함
    * namenode로 쓰이는 server 1, 2의 hdfs-site.xml에 아래 내용을 추가한다.

```
        <property>
                <name>dfs.namenode.shared.edits.dir</name>
                <value>qjournal://gbia-hdpalp-wb801:8485;gbia-hdpalp-wb803:8485;gbia-hdpalp-wb804:8485/alpha-dev</value>
        </property>
```

* dfs client가 active namenode를 찾기 위해 필요한 자바클래스를 명시한다.
    * server 1, 2, 3, 4의 hdfs-site.xml에 아래 내용을 추가한다.

```
        <property>
                <name>dfs.client.failover.proxy.provider.alpha-dev</name>
                <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
        </property>
```

* dfs client의 <span style="color:  #333333;;">default path prefix 정보 기입</span>
    * <span style="color:  #333333;;">server 1, 2, 3, 4의 core-site.xml에 아래 내용을 추가한다.</span>

```
        <property>
                <name>fs.defaultFS</name>
                <value>hdfs://alpha-dev</value>
        </property>
```

* failover 시 fencing을 위한 설정
    * server 1, 2의 hdfs-site.xml에 아래 내용 추가

```
        <property>
                <name>dfs.ha.fencing.methods</name>
                <value>sshfence</value>
        </property>

        <property>
                <name>dfs.ha.fencing.ssh.private-key-files</name>
                <value>/home1/irteam/.ssh/id_rsa</value>
        </property>
```

* journal node data를 저장할 디렉토리 생성 및 경로 설정
    * journal node 프로세스가 구동 될  server 1, 3, 4에 대해 아래 명령어 수행

```
mkdir -p /home1/irteam/hadoop-2.6.0/data/jn
```

* 소유권 변경
    * journalnode 기동 시 ([hadoop-daemon.sh](http://hadoop-daemon.sh/)start journalnode) sh 내부에서 해당 경로에 대한 쓰기 권한을 체크하는데, irteam 계정으로 기동 시킬 것임.
    * journal node 프로세스가 구동 될  server 1, 3, 4에 대해 아래 명령어 수행

```
chown irteam /home1/irteam/hadoop-2.6.0/data/jn
```

* server 1, 3, 4의 hdfs-site.xml에 아래 내용 추가

```
        <property>
                <name>dfs.journalnode.edits.dir</name>
                <value>/home1/irteam/hadoop-2.6.0/data/jn</value>
        </property>
```

* server 2에도 data 저장용 dir 미리 생성
    * 아래 명령어를 server 2에서 수행

```
mkdir -p /home1/irteam/hadoop-2.6.0/data
```

### NameNode daemon 설정

* namenode의 namespace와 transaction log 저장 dir 생성
    * server 1, 2 의 /home1/irteam/hadoop-2.6.0/data 경로에 nspace dir 생성

```
mkdir nspace
```

* 소유권 변경(server 1, 2)

```
chown irteam nspace
```

* namespace와 transaction log 저장 경로 설정
    * 아래 내용을 server 1, 2의 hdfs-site.xml에 추가

```
        <property>
                <name>dfs.namenode.name.dir</name>
                <value>/home1/irteam/hadoop-2.6.0/data/nspace</value>
        </property>
```

### DataNode daemon 설정

* datanode 가 block을 저장할 저장소의 경로 설정
    * server 3, 4의 /home1/irteam/hadoop-2.6.0/data 경로에 dn 디렉토리 생성
    * dn 디렉토리에 대한 소유권을 irteam으로 변경
    * server 3, 4의 hdfs-site.xml에 아래 내용 추가

```
        <property>
                <name>dfs.datanode.data.dir</name>
                <value>/home1/irteam/hadoop-2.6.0/data/dn</value>
        </property>
```

## .bash\_profile 관련 설정

* [hadoop-env.sh](http://hadoop-env.sh/)source
    * irteam 계정 로그인 시 마다 hadoop 관련 설정들을 반영시키기 위함
    * server 1, 2 3, 4의 irteam/.bash\_profile에 아래 내용 추가

```
source /home1/irteam/hadoop-2.6.0/etc/hadoop/hadoop-env.sh
```

* ZK\_HOME 설정
    * ZooKeeper 설치 경로 설정
    * hadoop 설정과 달리 ZooKeeper 설정은 많지 않아 irteam/.bash\_profile에서 바로 export 시키기로 함
    * server 1, 2 3, 4의 irteam/.bash\_profile에 아래 내용 추가

```
export ZK_HOME="/home1/irteam/apache-zookeeper-3.5.9-bin"
```

## [hadoop-env.sh](http://hadoop-env.sh/)설정

* HADOOP\_HOME DIR 설정

```
export HADOOP_HOME="/home1/irteam/hadoop-2.6.0"
```

* HADOOP\_CONF\_DIR 설정
    * HADOOP\_CONF\_DIR은 따로 설정하지 않을 시 기본적으로 /etc/hadoop으로 인식
    * namenode 기동 시 HADOOP\_CONF\_DIR 경로에서 core-site.xml을 찾게 됨
    * 나의 core-site.xml이 놓인 경로로 HADOOP\_CONF\_DIR을 변경한다.
    * 소유권을 irteam으로 변경한다.
    * server 1, 2, 3, 4에 대해 진행

```
export HADOOP_CONF_DIR="/home1/irteam/hadoop-2.6.0/etc/hadoop"
```

## ssh 설정

### 구성도
![image](https://user-images.githubusercontent.com/33619494/188458904-1abc7d47-8da6-4253-9e6b-c6020e592a9f.png)

* 구성도 설명
    * server 1, 2간에는 active 및 standby namenode 전환이 필요
    * namenode로 쓰이는 server 1, 2는 datanode로 쓰이는 3, 4에게 job명령을 수행시킴

### ssh를 위한 키 설정

* 위 구성도를 기준으로 ssh 키 설정을 진행한다.
* server 1, 2에서 아래 명령어 실행
    * 이후 요구되는 추가적인 입력에 대해서는 전부 enter로 넘어간다
        * 그렇게 될 경우 기본적으로 \~/.ssh 경로에 키가 생성 된다
        * 또한 비밀번호 없이 로그인하게 된다

```
ssh-keygen -t rsa
```

* server 1, 2의 \~/.ssh 경로에 authorized\_keys 파일을 만들어 그 안에 공개키 내용을 복사해 넣는다.
    * \~/.ssh 경로에서 아래 명령어 실행

```
cat id_rsa.pub >> authorized_keys
```

* server1, 2 간 공개키 등록
    * server1의 authorized\_keys 내부 내용을 복사해 server2의 authorized\_keys에 덧붙인다.
    * server2의 authorized\_keys 내부 내용을 복사해 server1의 authorized\_keys에 덧붙인다.
    * 결과적으로 server 1, 2의 authorized\_keys 에는 server 1의 id\_rsa.pub내용과 server 2의 id\_rsa.pub 내용이 들어있게 된다.
* server 3, 4에 공개키 등록
    * server 3, 4에 \~/.ssh 경로를 생성한다.
    * 위에서 등록한 server1의 authorized\_keys 내용을 복사한다.
    * server 3, 4의 \~/.ssh에서 아래 명령어 실행

```
echo "복사한내용붙여넣기" >> authorized_keys
```

* 권한 변경

server 1, 2, 3, 4의 authorized\_keys의 권한을 600으로 변경한다.

```
chmod 600 authorized_keys
```

* ssh 접속 시도
    * server 1, 2에서 ssh로 server 1,2,3,4  모두에 접속을 시도해 준다.
    * 첫 접속 시도 시 known\_host에 접속 정보를 추가한다는 확인을 받게 된다.
    * 이렇게 함으로써 \~/.ssh/known\_host 파일에 접속할 서버 정보가 추가되고 이후 접속 시에는 이를 묻지 않고 진행한다.

## 기타 설정

* logs 디렉토리 생성
    * $HADOOP\_HOME 경로에 logs 디렉토리 생성
    * 없을 경우 내부적으로 소스코드에서 mkdir 하지만, sudo 아닌 이상 permission  denied 될 수 있기에 미리 생성
    * chown으로 소유권 irteam으로 변경
    * server 1, 2, 3, 4에 진행

## 실행

* ZooKeeper 실행
    * server 2, 3, 4

```
$ZK_HOME/bin/zkServer.sh start
```

* journalnode 실행
    * server 1, 3, 4
    * namenode 기동 전 journalnode가 먼저 기동되어 있어야 함

```
$HADOOP_HOME/sbin/hadoop-daemon.sh start journalnode
```

* namenode 초기화
    * namenode01에 대해 진행
    * namenode format시 namespace(data/nn/current)와 journalnode 데이터(data/jn/current) 초기화 된다

```
$HADOOP_HOME/bin/hdfs namenode -format <cluster_name>
```

* ZooKeeper 초기화
    * server 1에서 진행

```
hdfs zkfc -formatZK
```

* namenode 실행
    * server1에서 실행

```
$HADOOP_HOME/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start namenode
```

* namenode metadata 복사
    * standbynamenode에서는 아래 명령어를 실행해 첫 번째 namenode의 metadata를 복사해와야한다.
    * server 2에서 실행
    * 이후 namenode02번도 기동시킨다.

```
$HADOOP_HOME/bin/hdfs namenode -bootstrapStandby
```

* zkfc 실행
    * server 1, 2 번에서 실행

```
$HADOOP_HOME/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start zkfc
```

* datanode 실행
    * server 3, 4

```
$HADOOP_HOME/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start datanode
```

* resourcemanager 실행
    * server1, 2

```
$HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start resourcemanager
```

* nodemanager 실행
    * server 3, 4

```
$HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start nodemanager
```

* namenode의 active or standby 상태 확인
    * ex) namenode01의 상태 확인하는 명령어

```
$HADOOP_HOME/bin/hdfs haadmin -getServiceState namenode01
```

## alias 설정

* 명령어 조작 용이성을 위해 모든 서버의 irteam/.bash\_profile에 아래 내용 추가했다.
* 관리의 편의성을 위해 별도 수정하지 않고 모든 서버에 적용 시킨다.
    * nodemanager를 기동시키지 않는 namenode에도 nodemanager 관련 alias가 들어가긴 하지만 사용하지 않으면 상관 없다.

```
PATH=$PATH:$HOME/.local/bin:$HOME/bin:$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$ZK_HOME/bin

export PATH

alias start-jn="hadoop-daemon.sh start journalnode"
alias stop-jn="hadoop-daemon.sh stop journalnode"

alias start-rm="yarn-daemon.sh --config $HADOOP_CONF_DIR start resourcemanager"
alias stop-rm="yarn-daemon.sh --config $HADOOP_CONF_DIR stop resourcemanager"

alias start-nm="yarn-daemon.sh --config $HADOOP_CONF_DIR start nodemanager"
alias stop-nm="yarn-daemon.sh --config $HADOOP_CONF_DIR stop nodemanager"

alias start-zkfc="hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start zkfc"
alias stop-zkfc="hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs stop zkfc"

alias format-nn="hdfs namenode -format alpha-dev"

alias start-nn="hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start namenode"
alias stop-nn="hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs stop namenode"

alias start-dn="hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start datanode"
alias stop-dn="hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs stop datanode"

alias status-nn="hdfs haadmin -getServiceState"
alias format-zkfc="hdfs zkfc -formatZK"

alias start-zk="bin/zkServer.sh start"
alias stop-zk="bin/zkServer.sh stop"

alias status-zk="./zkServer.sh status"
```